# ä»é›¶å®ç° Transformerï¼šè‹±è¯‘ä¸­ç¿»è¯‘ä»»åŠ¡

æœ¬é¡¹ç›®ä»é›¶å¼€å§‹å®ç°äº† Transformer æ¨¡å‹ï¼Œç”¨äºè‹±è¯­åˆ°ä¸­æ–‡çš„ç¿»è¯‘ä»»åŠ¡ã€‚

## ğŸ“– ç›®å½•

- [Transformer æ¶æ„æ¦‚è¿°](#transformer-æ¶æ„æ¦‚è¿°)
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
- [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#æ ¸å¿ƒç»„ä»¶è¯¦è§£)
  - [1. ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›](#1-ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›-scale-dot-product-attention)
  - [2. å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶](#2-å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶-multi-head-attention)
  - [3. å‰é¦ˆç¥ç»ç½‘ç»œ](#3-å‰é¦ˆç¥ç»ç½‘ç»œ-feed-forward-network)
  - [4. ä½ç½®ç¼–ç ](#4-ä½ç½®ç¼–ç -positional-encoding)
  - [5. Transformer Embedding](#5-transformer-embedding)
  - [6. ç¼–ç å™¨ Block](#6-ç¼–ç å™¨-block)
  - [7. è§£ç å™¨ Block](#7-è§£ç å™¨-block)
  - [8. å®Œæ•´ Transformer](#8-å®Œæ•´-transformer)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [é…ç½®å‚æ•°](#é…ç½®å‚æ•°)

---

## Transformer æ¶æ„æ¦‚è¿°

Transformer æ˜¯ 2017 å¹´ Google åœ¨è®ºæ–‡ [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762) ä¸­æå‡ºçš„é©å‘½æ€§æ¶æ„ã€‚å®ƒå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼ŒæŠ›å¼ƒäº†ä¼ ç»Ÿçš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼Œåœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚

### æ¶æ„å›¾ç¤º

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Output      â”‚
                    â”‚   Probabilities â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Linear      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                   â”‚         â”‚                   â”‚
    â”‚     Encoder       â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚     Decoder       â”‚
    â”‚   (N blocks)      â”‚  enc    â”‚   (N blocks)      â”‚
    â”‚                   â”‚  output â”‚                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    Embedding +    â”‚         â”‚    Embedding +    â”‚
    â”‚ Positional Encode â”‚         â”‚ Positional Encode â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Source Input  â”‚             â”‚ Target Input  â”‚
      â”‚   (English)   â”‚             â”‚   (Chinese)   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## é¡¹ç›®ç»“æ„

```
2-2/
â”œâ”€â”€ layers/                          # åŸºç¡€å±‚
â”‚   â”œâ”€â”€ scale_dot_product_attention.py  # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
â”‚   â”œâ”€â”€ multi_head_attention.py         # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
â”‚   â””â”€â”€ ffn.py                          # å‰é¦ˆç¥ç»ç½‘ç»œ
â”‚
â”œâ”€â”€ embedding/                       # åµŒå…¥å±‚
â”‚   â”œâ”€â”€ positional_encoding.py          # æ­£å¼¦ä½ç½®ç¼–ç 
â”‚   â””â”€â”€ transformer_embedding.py        # TokenåµŒå…¥ + ä½ç½®ç¼–ç 
â”‚
â”œâ”€â”€ blocks/                          # Transformer å—
â”‚   â”œâ”€â”€ encoder_block.py                # ç¼–ç å™¨ Block
â”‚   â”œâ”€â”€ encoder.py                      # å®Œæ•´ç¼–ç å™¨
â”‚   â”œâ”€â”€ decoder_block.py                # è§£ç å™¨ Block
â”‚   â””â”€â”€ decoder.py                      # å®Œæ•´è§£ç å™¨
â”‚
â”œâ”€â”€ transformer.py                   # Transformer å®Œæ•´æ¨¡å‹
â”œâ”€â”€ config.py                        # é…ç½®å‚æ•°
â”œâ”€â”€ text_process.py                  # æ–‡æœ¬é¢„å¤„ç†
â”œâ”€â”€ train.py                         # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ eval.py                          # æ¨ç†è„šæœ¬
â””â”€â”€ vocab.json                       # è¯è¡¨æ–‡ä»¶
```

---

## æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (Scale Dot-Product Attention)

> ğŸ“ æ–‡ä»¶ï¼š`layers/scale_dot_product_attention.py`

è¿™æ˜¯ Transformer ä¸­æœ€æ ¸å¿ƒçš„è®¡ç®—å•å…ƒã€‚æ³¨æ„åŠ›æœºåˆ¶çš„æœ¬è´¨æ˜¯ï¼š**æ ¹æ®æŸ¥è¯¢ï¼ˆQueryï¼‰å’Œé”®ï¼ˆKeyï¼‰çš„ç›¸ä¼¼åº¦ï¼Œå¯¹å€¼ï¼ˆValueï¼‰è¿›è¡ŒåŠ æƒæ±‚å’Œã€‚**

**æ•°å­¦å…¬å¼ï¼š**

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

å…¶ä¸­ $d_k$ æ˜¯é”®å‘é‡çš„ç»´åº¦ï¼Œé™¤ä»¥ $\sqrt{d_k}$ æ˜¯ä¸ºäº†é˜²æ­¢ç‚¹ç§¯ç»“æœè¿‡å¤§å¯¼è‡´ softmax æ¢¯åº¦æ¶ˆå¤±ã€‚

**ä»£ç å®ç°ï¼š**

```python
def forward(self, Q, K, V, mask=None):
    # Q, K, V å½¢çŠ¶: [batch_size, head, length, d_tensor]
    
    # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_tensor))
    
    # 2. åº”ç”¨æ©ç ï¼ˆç”¨äºè§£ç å™¨çš„å› æœé®è”½ï¼‰
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -10000)
    
    # 3. Softmax å½’ä¸€åŒ–
    attn_weights = torch.softmax(scores, dim=-1)
    
    # 4. åŠ æƒæ±‚å’Œ
    attn_output = torch.matmul(attn_weights, V)
    
    return attn_output
```

**å½¢çŠ¶å˜åŒ–ï¼š**
```
Q Ã— K^T: [B, H, L, D] Ã— [B, H, D, L] â†’ [B, H, L, L]  (æ³¨æ„åŠ›æƒé‡çŸ©é˜µ)
attn Ã— V: [B, H, L, L] Ã— [B, H, L, D] â†’ [B, H, L, D]  (åŠ æƒåçš„è¾“å‡º)
```

---

### 2. å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ (Multi-Head Attention)

> ğŸ“ æ–‡ä»¶ï¼š`layers/multi_head_attention.py`

å¤šå¤´æ³¨æ„åŠ›å…è®¸æ¨¡å‹åœ¨ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´ä¸­å¹¶è¡Œå­¦ä¹ ä¿¡æ¯ï¼Œå°±åƒåŒæ—¶ä»å¤šä¸ªè§’åº¦"å…³æ³¨"è¾“å…¥åºåˆ—ã€‚

**æ ¸å¿ƒæ€æƒ³ï¼š**
- å°† Qã€Kã€V åˆ†åˆ«æŠ•å½±åˆ°å¤šä¸ªä¸åŒçš„å­ç©ºé—´
- åœ¨æ¯ä¸ªå­ç©ºé—´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›
- å°†æ‰€æœ‰å¤´çš„è¾“å‡ºæ‹¼æ¥åï¼Œå†è¿›è¡Œä¸€æ¬¡çº¿æ€§å˜æ¢

**ä»£ç å®ç°ï¼š**

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, hidden_dim=512, num_heads=8):
        self.head_dim = hidden_dim // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦
        
        # å››ä¸ªæŠ•å½±çŸ©é˜µ
        self.W_q = nn.Linear(hidden_dim, hidden_dim)  # Query æŠ•å½±
        self.W_k = nn.Linear(hidden_dim, hidden_dim)  # Key æŠ•å½±
        self.W_v = nn.Linear(hidden_dim, hidden_dim)  # Value æŠ•å½±
        self.W_o = nn.Linear(hidden_dim, hidden_dim)  # è¾“å‡ºæŠ•å½±
    
    def forward(self, x_q, x_k, x_v, mask=None):
        # 1. çº¿æ€§æŠ•å½±
        Q = self.W_q(x_q)
        K = self.W_k(x_k)
        V = self.W_v(x_v)
        
        # 2. åˆ†å‰²å¤šå¤´ [B, L, D] â†’ [B, H, L, D/H]
        Q = self._split(Q)
        K = self._split(K)
        V = self._split(V)
        
        # 3. è®¡ç®—æ³¨æ„åŠ›
        attn_output = self.attention(Q, K, V, mask)
        
        # 4. æ‹¼æ¥å¤šå¤´å¹¶è¾“å‡ºæŠ•å½±
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, seq_len, hidden_dim)
        output = self.W_o(attn_output)
        
        return output
```

---

### 3. å‰é¦ˆç¥ç»ç½‘ç»œ (Feed-Forward Network)

> ğŸ“ æ–‡ä»¶ï¼š`layers/ffn.py`

FFN æ˜¯ä¸€ä¸ªç®€å•çš„ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œï¼Œç”¨äºå¯¹æ¯ä¸ªä½ç½®çš„è¡¨ç¤ºè¿›è¡Œéçº¿æ€§å˜æ¢ã€‚

$$\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2$$

```python
class FFN(nn.Module):
    def __init__(self, d_model, hidden, drop_prob=0.1):
        self.linear1 = nn.Linear(d_model, hidden)   # 512 â†’ 1024
        self.linear2 = nn.Linear(hidden, d_model)   # 1024 â†’ 512
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=drop_prob)

    def forward(self, x):
        x = self.linear1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return x
```

---

### 4. ä½ç½®ç¼–ç  (Positional Encoding)

> ğŸ“ æ–‡ä»¶ï¼š`embedding/positional_encoding.py`

ç”±äº Transformer æ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œæ— æ³•æ„ŸçŸ¥åºåˆ—ä¸­ token çš„ä½ç½®ä¿¡æ¯ã€‚**ä½ç½®ç¼–ç **é€šè¿‡ç»™æ¯ä¸ªä½ç½®æ·»åŠ ä¸€ä¸ªå›ºå®šçš„å‘é‡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

**æ­£å¼¦ä½ç½®ç¼–ç å…¬å¼ï¼š**

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

**ä»£ç å®ç°ï¼š**

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len, device):
        super().__init__()
        
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        self.encoding = torch.zeros(max_len, d_model, device=device)
        self.encoding.requires_grad = False  # ä¸å‚ä¸è®­ç»ƒ
        
        # ä½ç½®ç´¢å¼• [0, 1, 2, ..., max_len-1]
        pos = torch.arange(0, max_len, device=device).float().unsqueeze(1)
        
        # ç»´åº¦ç´¢å¼•
        k = torch.arange(0, d_model / 2, device=device).float()
        
        # è®¡ç®—æ­£å¼¦å’Œä½™å¼¦
        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (2 * k / d_model)))
        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (2 * k / d_model)))
```

**ä¸ºä»€ä¹ˆç”¨æ­£å¼¦/ä½™å¼¦ï¼Ÿ**
- å¯ä»¥è¡¨ç¤ºä»»æ„é•¿åº¦çš„åºåˆ—
- ç›¸å¯¹ä½ç½®å¯ä»¥é€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°
- å€¼åŸŸå›ºå®šåœ¨ [-1, 1]

---

### 5. Transformer Embedding

> ğŸ“ æ–‡ä»¶ï¼š`embedding/transformer_embedding.py`

å°† Token Embedding å’Œ Positional Encoding ç»“åˆï¼š

```python
class TransformerEmbedding(nn.Module):
    def __init__(self, vocab_size, d_model, max_len, drop_prob, pad_idx, device):
        # Token åµŒå…¥å±‚
        self.tok_emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)
        # ä½ç½®ç¼–ç 
        self.pos_emb = PositionalEncoding(d_model, max_len, device)
        # Dropout
        self.drop_out = nn.Dropout(p=drop_prob)

    def forward(self, x):
        tok_emb = self.tok_emb(x)           # [B, L] â†’ [B, L, D]
        pos_emb = self.pos_emb(x)           # [L, D]
        return self.drop_out(tok_emb + pos_emb)  # å¹¿æ’­ç›¸åŠ 
```

---

### 6. ç¼–ç å™¨ Block

> ğŸ“ æ–‡ä»¶ï¼š`blocks/encoder_block.py`

æ¯ä¸ªç¼–ç å™¨ Block åŒ…å«ä¸¤ä¸ªå­å±‚ï¼š
1. **å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚** (Multi-Head Self-Attention)
2. **å‰é¦ˆç¥ç»ç½‘ç»œå±‚** (Feed-Forward Network)

æ¯ä¸ªå­å±‚éƒ½æœ‰ **æ®‹å·®è¿æ¥** å’Œ **å±‚å½’ä¸€åŒ–**ã€‚

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      è¾“å…¥ x       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Multi-Head Attn  â”‚â—„â”€â”€ Self-Attention
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â–¼              â”‚
    â”‚    Dropout + LayerNorm     â”‚â—„â”€â”€ æ®‹å·®è¿æ¥
    â”‚              â”‚              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚       FFN         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â–¼              â”‚
    â”‚    Dropout + LayerNorm     â”‚â—„â”€â”€ æ®‹å·®è¿æ¥
    â”‚              â”‚              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      è¾“å‡º         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä»£ç å®ç°ï¼š**

```python
def forward(self, x):
    # 1. è‡ªæ³¨æ„åŠ›
    x_origin = x
    x = self.attention(x, x, x)  # Q=K=V=x (è‡ªæ³¨æ„åŠ›)
    
    # 2. æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
    x = self.dropout1(x)
    x = self.norm1(x + x_origin)
    
    # 3. å‰é¦ˆç½‘ç»œ
    x_origin = x
    x = self.ffn(x)
    
    # 4. æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
    x = self.dropout2(x)
    x = self.norm2(x + x_origin)
    
    return x
```

---

### 7. è§£ç å™¨ Block

> ğŸ“ æ–‡ä»¶ï¼š`blocks/decoder_block.py`

è§£ç å™¨ Block æ¯”ç¼–ç å™¨å¤šäº†ä¸€ä¸ª **Cross-Attention** å±‚ï¼Œç”¨äºå…³æ³¨ç¼–ç å™¨çš„è¾“å‡ºï¼š

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    ç›®æ ‡è¾“å…¥ x     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Masked Self-Attn â”‚â—„â”€â”€ å¸¦æ©ç çš„è‡ªæ³¨æ„åŠ›
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    Dropout + LayerNorm     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Cross-Attention â”‚â—„â”€â”€ Qæ¥è‡ªè§£ç å™¨ï¼ŒK/Væ¥è‡ªç¼–ç å™¨
    â”Œâ”€â”€â”€â”€â”‚                   â”‚â”€â”€â”€â”€â”
    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚              â”‚         Encoder Output
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    Dropout + LayerNorm     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚       FFN         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    Dropout + LayerNorm     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      è¾“å‡º         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®åŒºåˆ«ï¼š**
- **Masked Self-Attention**: ä½¿ç”¨ä¸‹ä¸‰è§’æ©ç ï¼Œé˜²æ­¢æ¨¡å‹"å·çœ‹"æœªæ¥çš„ token
- **Cross-Attention**: Query æ¥è‡ªè§£ç å™¨ï¼ŒKey å’Œ Value æ¥è‡ªç¼–ç å™¨è¾“å‡º

```python
def forward(self, x, enc, trg_mask):
    # 1. å¸¦æ©ç çš„è‡ªæ³¨æ„åŠ›
    x_origin = x
    x = self.self_attention(x, x, x, mask=trg_mask)  # æ·»åŠ æ©ç 
    x = self.dropout1(x)
    x = self.norm1(x + x_origin)
    
    # 2. Cross-Attentionï¼ˆç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›ï¼‰
    x_origin = x
    x = self.enc_dec_attention(x_q=x, x_k=enc, x_v=enc)  # Qæ¥è‡ªxï¼ŒK/Væ¥è‡ªenc
    x = self.dropout2(x)
    x = self.norm2(x + x_origin)
    
    # 3. å‰é¦ˆç½‘ç»œ
    x_origin = x
    x = self.ffn(x)
    x = self.dropout3(x)
    x = self.norm3(x + x_origin)
    
    return x
```

---

### 8. å®Œæ•´ Transformer

> ğŸ“ æ–‡ä»¶ï¼š`transformer.py`

å°†æ‰€æœ‰ç»„ä»¶ç»„åˆæˆå®Œæ•´çš„ Transformer æ¨¡å‹ï¼š

```python
class Transformer(nn.Module):
    def __init__(self, ...):
        # ç¼–ç å™¨
        self.encoder = Encoder(...)
        # è§£ç å™¨
        self.decoder = Decoder(...)
    
    def forward(self, src, trg):
        # 1. ç”Ÿæˆç›®æ ‡åºåˆ—çš„ä¸‹ä¸‰è§’æ©ç 
        mask = self.make_mask(trg)
        
        # 2. ç¼–ç æºåºåˆ—
        enc_src = self.encoder(src)
        
        # 3. è§£ç ç›®æ ‡åºåˆ—
        output = self.decoder(trg, enc_src, mask)
        
        return output
    
    def make_mask(self, trg):
        """ç”Ÿæˆä¸‹ä¸‰è§’æ©ç ï¼Œé˜²æ­¢æ¨¡å‹çœ‹åˆ°æœªæ¥çš„ token"""
        trg_len = trg.shape[1]
        trg_mask = torch.tril(torch.ones(trg_len, trg_len)).bool()
        return trg_mask.unsqueeze(0).repeat(trg.size(0), 1, 1)
```

---

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

```bash
pip install torch nltk
```

### è®­ç»ƒæ¨¡å‹

```bash
cd 2-2
python train.py
```

è®­ç»ƒè¿‡ç¨‹ä¼šï¼š
1. æ„å»ºè¯è¡¨å¹¶ä¿å­˜åˆ° `vocab.json`
2. ä½¿ç”¨å†…ç½®çš„è‹±è¯‘ä¸­æ•°æ®é›†è¿›è¡Œè®­ç»ƒ
3. å°†æ¨¡å‹ä¿å­˜åˆ° `transformer.pth`

### æ¨ç†æµ‹è¯•

```bash
python eval.py
```

ç„¶åè¾“å…¥è‹±æ–‡å¥å­ï¼Œæ¨¡å‹ä¼šè¾“å‡ºå¯¹åº”çš„ä¸­æ–‡ç¿»è¯‘ï¼š

```
Using device: mps
Input english: Good morning!
Output chinese: <bos>æ—©ä¸Šå¥½ï¼<eos>
```

---

## é…ç½®å‚æ•°

> ğŸ“ æ–‡ä»¶ï¼š`config.py`

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `d_model` | 512 | æ¨¡å‹/åµŒå…¥ç»´åº¦ |
| `n_head` | 8 | å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•° |
| `max_len` | 40 | æœ€å¤§åºåˆ—é•¿åº¦ |
| `ffn_hidden` | 1024 | FFN éšè—å±‚ç»´åº¦ |
| `n_blocks` | 2 | ç¼–ç å™¨/è§£ç å™¨çš„å±‚æ•° |
| `drop_prob` | 0.1 | Dropout æ¦‚ç‡ |
| `batch_size` | 20 | æ‰¹æ¬¡å¤§å° |
| `lr` | 0.001 | å­¦ä¹ ç‡ |
| `epochs` | 200 | è®­ç»ƒè½®æ•° |

---

## æ–‡æœ¬å¤„ç†æµç¨‹

> ğŸ“ æ–‡ä»¶ï¼š`text_process.py`

```
è¾“å…¥: "Hello World!"
    â†“ tokenize (åˆ†è¯)
['Hello', 'World', '!']
    â†“ add_special_token (æ·»åŠ ç‰¹æ®Šæ ‡è®°)
['<bos>', 'Hello', 'World', '!', '<eos>', '<pad>', '<pad>', ...]
    â†“ vocab lookup (è¯è¡¨æ˜ å°„)
[0, 45, 78, 12, 1, 2, 2, ...]
    â†“ to tensor
tensor([0, 45, 78, 12, 1, 2, 2, ...])
```

**ç‰¹æ®Š Tokenï¼š**
- `<bos>`: åºåˆ—å¼€å§‹æ ‡è®° (Beginning of Sequence)
- `<eos>`: åºåˆ—ç»“æŸæ ‡è®° (End of Sequence)
- `<pad>`: å¡«å……æ ‡è®° (Padding)
- `<unk>`: æœªçŸ¥è¯æ ‡è®° (Unknown)

---

## è®­ç»ƒæŠ€å·§

### Teacher Forcing

è®­ç»ƒæ—¶ä½¿ç”¨ **Teacher Forcing** ç­–ç•¥ï¼šè§£ç å™¨çš„è¾“å…¥æ˜¯çœŸå®çš„ç›®æ ‡åºåˆ—ï¼ˆå»æ‰æœ€åä¸€ä¸ª tokenï¼‰ï¼Œè€Œä¸æ˜¯æ¨¡å‹è‡ªå·±çš„é¢„æµ‹ç»“æœã€‚

```python
# target[:, :-1] ä½œä¸ºè§£ç å™¨è¾“å…¥ï¼ˆå»æ‰ <eos>ï¼‰
output = model(input, target[:, :-1])

# target[:, 1:] ä½œä¸ºè®­ç»ƒæ ‡ç­¾ï¼ˆå»æ‰ <bos>ï¼‰
loss = criterion(output.reshape(-1, vocab_size), target[:, 1:].reshape(-1))
```

### è‡ªå›å½’ç”Ÿæˆ

æ¨ç†æ—¶ä½¿ç”¨ **è‡ªå›å½’ç”Ÿæˆ**ï¼šæ¯æ¬¡ç”Ÿæˆä¸€ä¸ª tokenï¼Œå¹¶å°†å…¶åŠ å…¥åˆ°è§£ç å™¨è¾“å…¥ä¸­ã€‚

```python
target = torch.tensor([[vocab['<bos>']]])  # åˆå§‹åŒ–ä¸º <bos>

for _ in range(max_len - 1):
    output = model(input_data, target)
    next_token = output[:, -1, :].argmax(dim=-1)  # å–æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹
    target = torch.cat([target, next_token.unsqueeze(1)], dim=1)
    
    if next_token.item() == vocab['<eos>']:
        break
```

---

Happy Learning! ğŸš€
